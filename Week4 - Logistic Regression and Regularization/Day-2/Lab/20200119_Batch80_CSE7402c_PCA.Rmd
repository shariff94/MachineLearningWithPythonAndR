---
title: "Principal Component Analysis Activity"
author: "INSOFE Lab Activity on PCA"
date: "19 Jan 2020"
output:
  html_document
---

**NOTE** Before starting this assignment please remember to clear your environment, you can do that by running the following code chunk

```{r}

rm(list = ls(all=TRUE))

```

# Agenda

* Understand PCA from first principles

* Understand the Importance of Data Scaling in PCA

* Automated computation of the Principal Components using packages

* Apply PCA for data complexity reduction

# Iris Data

**Data Description:**

- The data set contains 5 variables and 3 classes of 50 instances each, where each class refers to a type of iris plant.

- The variable names are self explanatory

- This dataset was chosen for the purposes of making the concepts and application of PCA clear.

## Understand the data

* Read in the data

```{r}
iris_data <- iris

```



* Get the structure and summary of the data

* The data has 5 attributes and 150 rows

```{r}

str(iris_data)

summary(iris_data)

```

Running the multiclass classification model on the iris dataset

```{r}

library(nnet)

multinom_model = multinom(Species~., data = iris_data )


pred_results = multinom_model$fitted.values


actl_results = iris_data$Species


results = data.frame("pred_spec" = colnames(pred_results)[apply(pred_results,1,function(x) which(x==max(x)))], 
                     "act_spec" = iris_data$Species)

table(results)


```



## Linear Separability in the data

* Compute the variance of each variable in the dataset

* Plot the data across the two variables with the highest variance

```{r}

# Using lapply() function we apply the var() function on each of the variables excluding the target

lapply(iris_data[, -5], var)

# Plot the data points on the axes with the highest variances

plot(iris_data$Sepal.Length, iris_data$Petal.Length, col = iris_data$Species, xlab = "Sepal Length", ylab = "Petal Length",
    main = "Linear Separability before PCA")



```

* From the above plot it is visible that the data cannot be linearly separated by just using two axes, that too the ones with the highest variance


Automated approach to implement the Principle components (as below).

```{r}

pca_princomp <- princomp(iris_data[,-c(5)])
summary(pca_princomp)
pca_princomp$loadings
head(pca_princomp$scores)
plot(pca_princomp)

iris_pca_data = data.frame(pca_princomp$scores, Species = iris_data$Species)

plot(iris_pca_data$Comp.1, iris_pca_data$Comp.2, col = iris_pca_data$Species, xlab = "Principal Component 1", ylab = "Principal Component 2",  main = "Linear Separability after PCA")


reg_mod_2 = multinom(Species~., data = iris_pca_data )

pred_results_2 = reg_mod_2$fitted.values


actl_results_2 = iris_pca_data$Species


results_prcom = data.frame("pred_spec" = colnames(pred_results_2)[apply(pred_results_2,1,function(x) which(x==max(x)))], 
                     "act_spec" = iris_pca_data$Species)

table(results_prcom)

```


########################################################################################################################

Automated approach to implement the Principle components with scaling (as below).
```{r }

pca_scale <- princomp(iris_data[,-c(5)], cor = T)
summary(pca_scale)

plot(pca_scale)

iris_pca_Scale_data = data.frame(pca_scale$scores, Species = iris_data$Species)

plot(iris_pca_Scale_data$Comp.1, iris_pca_Scale_data$Comp.2, col = iris_pca_Scale_data$Species, xlab = "Principal Component 1", ylab = "Principal Component 2",  main = "Linear Separability after PCA")


reg_mod_3 = multinom(Species~., data = iris_pca_Scale_data )

pred_results_3 = reg_mod_3$fitted.values


actl_results_3 = iris_pca_Scale_data$Species


results_prcom = data.frame("pred_spec" = colnames(pred_results_3)[apply(pred_results_3,1,function(x) which(x==max(x)))], 
                     "act_spec" = iris_pca_Scale_data$Species)

table(results_prcom)

```

# Data Pre-processing

## Split the data into train and test

We have to remove the state variable, as it has very low information content

* 80/20 split of train and test

```{r}

set.seed(420)

train_rows <- sample(1:nrow(iris_data), 0.8*nrow(iris_data))

train_data <- iris_data[train_rows, ]

test_data <- iris_data[-train_rows, ]

```


# Automated Computation of Principal Components



## Sclaed PCA computation

* Use the princomp() function to get the scaled principle components as it has two additional arguments to do so

* Remove the "Species" variable while doing so

```{r}

pca_scaled <- princomp(train_data[, !(names(train_data) %in% c("Species"))], cor = T)

head(pca_scaled$scores)

```

* Plot the variance along each of the principal components

* We can now see that the dominance of the variance along the first principal component has reduced significantly

```{r}

plot(pca_scaled)

```


* Hence scaling the data before PCA, is extremely important as the higher range of one variable might unfairly influence the principal components

# Apply PCA on the Original Data

* Project the train and test data sets onto the derived principal components

```{r}

train_pca_e <- as.data.frame(predict(pca_scaled, train_data[, !(names(train_data) %in% c("Species"))]))

test_pca_e <- as.data.frame(predict(pca_scaled, test_data[, !(names(train_data) %in% c("Species"))]))

```


